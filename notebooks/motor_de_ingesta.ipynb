{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8e4950ded27f59",
   "metadata": {},
   "source": [
    "### Implementacion del motor de ingesta\n",
    "\n",
    "Se ha compartido un archivo wheel el cual debe ser instalado en el computo del cluster, aquí procedemos a instalarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f4117aa8cd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall ruta/motor_ingesta-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec75c7fc69ce3ac",
   "metadata": {},
   "source": [
    "Recordar que para esta parte se deben tener creados dos nuevos blob containers:\n",
    "\n",
    "1. landingcaso\n",
    "2. datalakecaso\n",
    "\n",
    "Ya que en estos se realizará la ejecución de todo lo necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3646a49cbd56248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion de sesion de spark - Solo desarrollo\n",
    "\n",
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38201010",
   "metadata": {},
   "source": [
    "#### Import para el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac49a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "\n",
    "from MotorIngesta.batch_ingestion import batch_ingestion\n",
    "from MotorIngesta.streaming_ingestion import streaming_ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e3de2",
   "metadata": {},
   "source": [
    "#### Archivo de configuración, lectura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36672c0d",
   "metadata": {},
   "source": [
    "## Ingesta de datos batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T03:23:35.909093Z",
     "start_time": "2025-05-21T03:23:35.671942Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://landingcaso@datalakestoragecasoucm01.dfs.core.windows.net\n",
      "abfss://lakehousecaso@datalakestoragecasoucm01.dfs.core.windows.net/raw\n",
      "abfss://lakehousecaso@datalakestoragecasoucm01.dfs.core.windows.net/bronze\n",
      "abfss://landingcaso@datalakestoragecasoucm01.dfs.core.windows.net/retail/sales_orders\n",
      "abfss://lakehousecaso@datalakestoragecasoucm01.dfs.core.windows.net/raw/retail/sales_orders\n",
      "abfss://lakehousecaso@datalakestoragecasoucm01.dfs.core.windows.net/bronze/retail/sales_orders\n"
     ]
    }
   ],
   "source": [
    "format = \"json\"\n",
    "\n",
    "account = spark.conf.get(\"adls.account.name\")\n",
    "\n",
    "datasource = 'retail'\n",
    "dataset = \"sales_orders\"\n",
    "\n",
    "landing_container = f\"abfss://landingcaso@{account}.dfs.core.windows.net\"\n",
    "lakehouse_container = f\"abfss://lakehousecaso@{account}.dfs.core.windows.net\"\n",
    "\n",
    "landing_path = landing_container\n",
    "raw_path = f\"{lakehouse_container}/raw\"\n",
    "bronze_path = f\"{lakehouse_container}/bronze\"\n",
    "\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "print(landing_path)\n",
    "print(raw_path)\n",
    "print(bronze_path)\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b18b58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LandingStreamReader(datasource='retail',dataset='sales_orders')\n",
      "BronzeStreamWriter(datasource='retail',dataset='sales_orders')\n"
     ]
    }
   ],
   "source": [
    "batch_ingestion(datasource, dataset, landing_path, raw_path, bronze_path, format, spark, dbutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e3064",
   "metadata": {},
   "source": [
    "## Ingesta de datos desde Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"mobile_client\"\n",
    "datasource = \"mobile_client_events\"\n",
    "dataset = topic\n",
    "\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_checkpoint_path = f\"{bronze_path}/{datasource}/{dataset}_checkpoint\"\n",
    "table_name = f\"hive_metastore.bronze.{datasource}_{dataset}\"\n",
    "\n",
    "config_dict = read_config('config.json')\n",
    "print(config_dict)\n",
    "\n",
    "schema_registry_url = config_dict[\"schema_registry_url\"]\n",
    "schema_registry_username = config_dict[\"schema_registry_username\"]\n",
    "schema_registry_password = config_dict[\"schema_registry_password\"]\n",
    "\n",
    "schema_registry_conf = {'url': schema_registry_url,\n",
    "                        'basic.auth.user.info' : f'{schema_registry_username}:{schema_registry_password}'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f103771",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_ingestion(topic, dataset_bronze_path, dataset_bronze_checkpoint_path, table_name, spark, schema_registry_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
