{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29cc3164-a814-4cb5-a6f3-e8693cfd37a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generación de archivos\n",
    "\n",
    "Vamos a usar algunos datasets por defecto por practicidad, primero realizamos los import del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2350ff-3e26-4d61-92cc-8a56becaa672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f0138d-f10c-4077-bd09-a0a4e3dccacc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "organization = 'FarmIA' # Folder raíz en el que quedaran los files\n",
    "\n",
    "account = spark.conf.get(\"adls.account.name\")\n",
    "landing_path = f\"abfss://landingcaso@{account}.dfs.core.windows.net\"\n",
    "\n",
    "print(landing_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb053e3d-3962-453e-9015-f55705e4f454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reutilizaremos la misma funcion para enviar en la misma estructura de año, mes y día los archivos a cada ruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c679ff50-8414-4488-8a3c-4e36d58f636a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def land_file(df,datasource,dataset,format='json'):\n",
    "  \"\"\"\n",
    "    Guarda un DataFrame en un sistema de archivos distribuido con una estructura de directorios basada en la fecha actual,\n",
    "    utilizando un formato específico (por defecto, JSON). La función escribe el DataFrame en una ubicación temporal,\n",
    "    lo mueve a una ruta final organizada por fuente de datos, conjunto de datos y marca de tiempo, y luego elimina el\n",
    "    directorio temporal.\n",
    "\n",
    "    Parámetros:\n",
    "        df (pyspark.sql.DataFrame): El DataFrame de Spark que se desea guardar.\n",
    "        datasource (str): Nombre o identificador de la fuente de datos, usado para organizar la ruta final.\n",
    "        dataset (str): Nombre o identificador del conjunto de datos, usado para organizar la ruta final.\n",
    "        format (str, opcional): Formato en el que se guardará el archivo. Por defecto es 'json'. \n",
    "                                Otros formatos soportados dependen de Spark (e.g., 'parquet', 'csv').\n",
    "\n",
    "    Comportamiento:\n",
    "        1. Escribe el DataFrame en una carpeta temporal (`tmp_path`) usando el formato especificado, coalesciendo los datos en un solo archivo.\n",
    "        2. Genera una ruta final basada en la fecha actual (`YYYY/MM/DD`), el nombre de la fuente de datos, el conjunto de datos y una marca de tiempo.\n",
    "        3. Mueve el archivo generado desde la carpeta temporal a la ruta final.\n",
    "        4. Imprime la ruta final del archivo guardado.\n",
    "        5. Elimina la carpeta temporal.\n",
    "\n",
    "    Variables externas utilizadas:\n",
    "        - landing_path (str): Ruta base del sistema de archivos donde se almacenan los datos. Debe estar definida globalmente.\n",
    "        - dbutils.fs: Utilidad de Databricks para manipular el sistema de archivos (ls, mv, rm).\n",
    "        - datetime: Módulo de Python para manejar fechas y marcas de tiempo.\n",
    "\n",
    "    Ejemplo:\n",
    "        save_file(mi_dataframe, \"ventas\", \"diarias\", format=\"parquet\")\n",
    "        # Salida esperada: \"dbfs:/landing/ventas/diarias/2025/03/14/ventas_diarias_20250314123045.parquet\"\n",
    "\n",
    "    Notas:\n",
    "        - La función asume que está ejecutándose en un entorno compatible con Databricks (por el uso de `dbutils.fs`).\n",
    "        - Si el formato especificado no es compatible con Spark, se generará un error.\n",
    "    \"\"\"\n",
    "  tmp_path = f'{landing_path}/tmp/'\n",
    "  df.coalesce(1).write.format(format).mode(\"overwrite\").save(tmp_path)\n",
    "  now = datetime.datetime.utcnow()\n",
    "  date_path = now.strftime(\"%Y/%m/%d\")\n",
    "  timestamp = now.strftime(\"%Y%m%d%H%M%S\") \n",
    "  for file in dbutils.fs.ls(tmp_path):\n",
    "    if file.name.endswith(f'.{format}'):\n",
    "      final_path = file.path.replace('tmp',f'{datasource}/{dataset}')\n",
    "      final_path = final_path.replace(file.name, f'{date_path}/{datasource}-{dataset}-{timestamp}.{format}')\n",
    "      dbutils.fs.mv(file.path, final_path)\n",
    "      print(final_path)\n",
    "  dbutils.fs.rm(tmp_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d03cef8-4c1f-40d0-bf7b-f791515dd03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datos de sensores de IoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd6b3dc-998a-4905-a573-f0867cfc3adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset origen: dbfs:/databricks-datasets/iot-stream/\n",
    "# display(dbutils.fs.ls(f\"/databricks-datasets/iot-stream/data-device/\"))\n",
    "\n",
    "iot_df = spark.read.json(f\"/databricks-datasets/iot-stream/data-device/\")\n",
    "# iot_df.printSchema()\n",
    "\n",
    "# Generar archivos de id con valores random cada vez y cargarlo a landing\n",
    "\n",
    "min_value = random.randint(0, 999999)\n",
    "max_value = min_value + random.randint(0, 1000)\n",
    "\n",
    "print(f\"Mínimo: {min_value}, Máximo: {max_value}\")\n",
    "\n",
    "file = iot_df.where(f\"id between {min_value} and {max_value}\").coalesce(1)\n",
    "\n",
    "land_file(file,'FarmIA','iot-devices')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74de28e0-c582-4c19-9c3a-b03fe0527ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datos de ventas de clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6333a276-22b6-4544-97c6-959b9f1bae91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasource = \"retail-org\"\n",
    "dataset = \"sales_orders\"\n",
    "\n",
    "# Lectura de dataset de sales order\n",
    "\n",
    "df = spark.read.json(f\"/databricks-datasets/{datasource}/{dataset}/\")\n",
    "# df.printSchema()\n",
    "\n",
    "# Generar archivos de sales_oder con valores random cada vez y cargarlo a landing\n",
    "\n",
    "min_value = random.randint(0, 90000000)\n",
    "max_value = min_value + random.randint(0, 10000)\n",
    "\n",
    "print(f\"Mínimo: {min_value}, Máximo: {max_value}\")\n",
    "\n",
    "file = df.where(f\"customer_id between {min_value} and {max_value}\").coalesce(1)\n",
    "\n",
    "land_file(file,'FarmIA','sales_orders')\n",
    "time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "files_generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
